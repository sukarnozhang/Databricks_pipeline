{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9749d36-2a78-496f-a0a8-00fa10a809ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "import boto3\n",
    "\n",
    "# Step 1 - Establish AWS connections\n",
    "def establish_aws_connection():\n",
    "    # Step 1: Get AWS credentials from Databricks Secrets\n",
    "    aws_access_key = dbutils.secrets.get(scope=\"aws-secrets\", key=\"aws-access-key\")\n",
    "    aws_secret_key = dbutils.secrets.get(scope=\"aws-secrets\", key=\"aws-secret-key\")\n",
    "\n",
    "    # Step 2: Initialize boto3 client for S3\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key,\n",
    "        aws_secret_access_key=aws_secret_key\n",
    "    )\n",
    "\n",
    "    return s3\n",
    "\n",
    "# Step 2\n",
    "def load_config_txt(path):\n",
    "    config = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Skip empty lines and comments\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            # Split key=value\n",
    "            if '=' in line:\n",
    "                key, value = line.split(\"=\", 1)\n",
    "                config[key.strip()] = value.strip()\n",
    "    return config\n",
    "\n",
    "\n",
    "# Step 2 - create tracking tables for files which have been ingested\n",
    "def create_table(table_name, schema):\n",
    "\n",
    "    # Create an empty DataFrame with this schema\n",
    "    empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "    # Check if table exists\n",
    "    if not spark.catalog.tableExists(table_name):\n",
    "        empty_df.write.format(\"delta\").saveAsTable(table_name)\n",
    "        print(\"Tracking table created.\")\n",
    "    else:\n",
    "        print(\"Tracking table already exists.\")\n",
    "\n",
    "# Step 3: Get already ingested file names\n",
    "def get_ingested_files(tracking_table):\n",
    "    try:\n",
    "        ingested_files = set(row[\"file_name\"] for row in spark.table(tracking_table).collect())\n",
    "    except:\n",
    "        ingested_files = set()\n",
    "        print(\"Tracking table not found. Assuming no files ingested yet.\")\n",
    "    return ingested_files"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "processor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
