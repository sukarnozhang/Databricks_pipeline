{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9749d36-2a78-496f-a0a8-00fa10a809ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime, timezone\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "import boto3\n",
    "\n",
    "# Step 1 - Establish AWS connections\n",
    "def establish_aws_connection():\n",
    "    # Step 1: Get AWS credentials from Databricks Secrets\n",
    "    aws_access_key = dbutils.secrets.get(scope=\"aws-secrets\", key=\"aws-access-key\")\n",
    "    aws_secret_key = dbutils.secrets.get(scope=\"aws-secrets\", key=\"aws-secret-key\")\n",
    "\n",
    "    # Step 2: Initialize boto3 client for S3\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key,\n",
    "        aws_secret_access_key=aws_secret_key\n",
    "    )\n",
    "\n",
    "    return s3\n",
    "\n",
    "# Step 2\n",
    "def load_config_txt(path):\n",
    "    config = {}\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Skip empty lines and comments\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            # Split key=value\n",
    "            if '=' in line:\n",
    "                key, value = line.split(\"=\", 1)\n",
    "                config[key.strip()] = value.strip()\n",
    "    return config\n",
    "\n",
    "\n",
    "# Step 2 - create tracking tables for files which have been ingested\n",
    "def create_table(table_name, schema):\n",
    "\n",
    "    # Create an empty DataFrame with this schema\n",
    "    empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "    # Check if table exists\n",
    "    if not spark.catalog.tableExists(table_name):\n",
    "        empty_df.write.format(\"delta\").saveAsTable(table_name)\n",
    "        print(\"Tracking table created.\")\n",
    "\n",
    "\n",
    "        # Special handling: insert very old timestamp for last_ingested_times\n",
    "        if table_name == \"workspace.silver_schema.last_ingested_times\":\n",
    "            very_old_timestamp = datetime(1900, 1, 1, 0, 0, 0)\n",
    "            placeholder_row = Row(last_ingested_times=very_old_timestamp)\n",
    "            init_df = spark.createDataFrame([placeholder_row], schema)\n",
    "            init_df.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
    "            print(\"Inserted very old timestamp into last_ingested_times.\")\n",
    "    else:\n",
    "        print(\"Tracking table already exists.\")\n",
    "\n",
    "# Step 3: Get already ingested file names\n",
    "def get_ingested_files(tracking_table):\n",
    "    try:\n",
    "        ingested_files = set(row[\"file_name\"] for row in spark.table(tracking_table).collect())\n",
    "    except:\n",
    "        ingested_files = set()\n",
    "        print(\"Tracking table not found. Assuming no files ingested yet.\")\n",
    "    return ingested_files\n",
    "\n",
    "\n",
    "# step 4: get s3 data\n",
    "def get_s3_data(s3, bucket, prefix, files_tracking, main_schema, last_ingested_times, files_tracking_schema):\n",
    "    all_data = [] # to store all new data from json\n",
    "    new_ingested_json = [] \n",
    "    try:\n",
    "        response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "        for obj in response.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            last_modified = obj['LastModified']  # this is a datetime object in UTC\n",
    "            print(\"hello_world\")\n",
    "            # Only ingest files modified after the last_ingested_times\n",
    "            if key.endswith('.json') and last_modified > last_ingested_times:\n",
    "                # Download and parse\n",
    "                file_obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "                json_data = file_obj['Body'].read().decode('utf-8')\n",
    "                data_dict = json.loads(json_data)\n",
    "                all_data.append(data_dict)\n",
    "\n",
    "                # Log the file for tracking\n",
    "                new_ingested_json.append((key, datetime.now(timezone.utc)))\n",
    "\n",
    "            if new_ingested_json:\n",
    "                latest_modified_time = max(modified_time for _, modified_time in new_ingested_json)\n",
    "\n",
    "                updated_row = Row(last_ingested_times=latest_modified_time)\n",
    "                updated_df = spark.createDataFrame([updated_row])\n",
    "\n",
    "                updated_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"workspace.silver_schema.last_ingested_times\")\n",
    "\n",
    "                print(f\"Updated last_ingested_times to {latest_modified_time}\")\n",
    "            else:\n",
    "                print(\"No new files ingested. Timestamp not updated.\")\n",
    "\n",
    "        print(f\"New JSON files to ingest: {len(new_ingested_json)}\")\n",
    "\n",
    "    except NoCredentialsError:\n",
    "        print(\"AWS credentials not found!\")\n",
    "        all_data = []\n",
    "\n",
    "\n",
    "    if all_data:\n",
    "        silver_df = spark.createDataFrame(all_data, schema=main_schema)\n",
    "\n",
    "        # Append new data\n",
    "        silver_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"workspace.silver_schema.silver_delta_table\")\n",
    "        print(\"New data appended to Delta table.\")\n",
    "\n",
    "        # Step 7: Update tracking table\n",
    "        print(\"new files is:\", new_ingested_json)\n",
    "        new_ingested_json_df = spark.createDataFrame(new_ingested_json, schema=files_tracking_schema)\n",
    "        new_ingested_json_df.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .option(\"mergeSchema\", \"true\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(files_tracking)\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"No new files to process.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "processor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
